{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save processed speech command data as wav file\n",
    "save_processed_speech_cmd_files = False\n",
    "# Use data augmentation\n",
    "use_augmentation = True\n",
    "# whether to train on whole dataset \n",
    "TRAIN_RAKSHAK_ON_ALL_DATA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = 'rakshak'\n",
    "TEST = ['speech_cmd', 'rakshak']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_path = \"../input/train/audio/\"\n",
    "speech_files = ['go', 'stop', 'yes', 'no']\n",
    "background_noises = ['dude_miaowing.wav','pink_noise.wav','white_noise.wav','doing_the_dishes.wav','exercise_bike.wav','running_tap.wav']\n",
    "speech_setup = 'speech_setup/'\n",
    "speech_setup_data = speech_path+'data/'\n",
    "rakshak_path = 'rakshak/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kapre\r\n",
      "  Downloading https://files.pythonhosted.org/packages/3f/2e/f540d1d1f05c764686163fdb5bb1e5c703f1528076d2829bfc3900683f06/kapre-0.1.4-py3-none-any.whl\r\n",
      "Collecting tensorflow-gpu\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\r\n",
      "\u001b[K     |████████████████████████████████| 377.0MB 49kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: keras>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from kapre) (2.2.4)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.6/site-packages (from kapre) (0.17.1)\r\n",
      "Requirement already satisfied: librosa>=0.5 in /opt/conda/lib/python3.6/site-packages (from kapre) (0.7.0)\r\n",
      "Requirement already satisfied: numpy>=1.8.0 in /opt/conda/lib/python3.6/site-packages (from kapre) (1.16.4)\r\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (3.7.1)\r\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (1.14.0)\r\n",
      "Requirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (0.2.2)\r\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (1.22.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (1.1.0)\r\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (1.12.0)\r\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (1.14.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (1.11.2)\r\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (0.7.1)\r\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (0.8.0)\r\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (1.0.8)\r\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (1.1.0)\r\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (0.33.4)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.6/site-packages (from tensorflow-gpu) (0.1.7)\r\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from keras>=2.0.0->kapre) (1.2.1)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from keras>=2.0.0->kapre) (2.9.0)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from keras>=2.0.0->kapre) (5.1.1)\r\n",
      "Requirement already satisfied: soundfile>=0.9.0 in /opt/conda/lib/python3.6/site-packages (from librosa>=0.5->kapre) (0.10.2)\r\n",
      "Requirement already satisfied: resampy>=0.2.0 in /opt/conda/lib/python3.6/site-packages (from librosa>=0.5->kapre) (0.2.1)\r\n",
      "Requirement already satisfied: decorator>=3.0.0 in /opt/conda/lib/python3.6/site-packages (from librosa>=0.5->kapre) (4.4.0)\r\n",
      "Requirement already satisfied: numba>=0.38.0 in /opt/conda/lib/python3.6/site-packages (from librosa>=0.5->kapre) (0.44.1)\r\n",
      "Requirement already satisfied: joblib>=0.12 in /opt/conda/lib/python3.6/site-packages (from librosa>=0.5->kapre) (0.13.2)\r\n",
      "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /opt/conda/lib/python3.6/site-packages (from librosa>=0.5->kapre) (0.21.2)\r\n",
      "Requirement already satisfied: audioread>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from librosa>=0.5->kapre) (2.1.8)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-gpu) (41.0.1)\r\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (0.15.4)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (3.1.1)\r\n",
      "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.6/site-packages (from soundfile>=0.9.0->librosa>=0.5->kapre) (1.12.3)\r\n",
      "Requirement already satisfied: llvmlite>=0.29.0 in /opt/conda/lib/python3.6/site-packages (from numba>=0.38.0->librosa>=0.5->kapre) (0.29.0)\r\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.6/site-packages (from cffi>=1.0->soundfile>=0.9.0->librosa>=0.5->kapre) (2.19)\r\n",
      "Installing collected packages: kapre, tensorflow-gpu\r\n",
      "Successfully installed kapre-0.1.4 tensorflow-gpu-1.14.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install kapre tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'rakshak'...\r\n",
      "remote: Enumerating objects: 337, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (337/337), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (335/335), done.\u001b[K\r\n",
      "remote: Total 5676 (delta 9), reused 328 (delta 2), pack-reused 5339\r\n",
      "Receiving objects: 100% (5676/5676), 406.24 MiB | 20.73 MiB/s, done.\r\n",
      "Resolving deltas: 100% (415/415), done.\r\n",
      "Checking out files: 100% (4994/4994), done.\r\n"
     ]
    }
   ],
   "source": [
    "# put dataset in folder named rakshak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "random.seed(4)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backend_bases import RendererBase\n",
    "\n",
    "import plotly as py\n",
    "py.tools.set_credentials_file(username='Ujjwal999',api_key='QNq0B1RHjBetoHBqLOjy')\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import cufflinks as cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_processed_speech_cmd_files:\n",
    "    if not os.path.isdir(speech_setup_data):\n",
    "        os.mkdir(speech_setup_data)\n",
    "    for dir_ in speech_files: \n",
    "        if not os.path.isdir(speech_setup_data+dir_):\n",
    "            os.mkdir(speech_setup_data+dir_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll(data):\n",
    "    data_roll = np.roll(data, 5000)\n",
    "    return data_roll\n",
    "\n",
    "def stretch(data, rate=2):\n",
    "    input_length = 16000*3\n",
    "    data = librosa.effects.time_stretch(np.array(data, dtype=np.float32), rate)\n",
    "    if len(data)>input_length:\n",
    "        data = data[:input_length]\n",
    "    else:\n",
    "        data = np.pad(data, (0, max(0, input_length - len(data))), \"constant\")\n",
    "    return data.tolist()\n",
    "\n",
    "def wnoise(data):\n",
    "    wn = np.random.randn(len(data))\n",
    "    data_wn = data + 0.005*wn\n",
    "    return data_wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def make_3_second(path, files):\n",
    "    r = random.random()\n",
    "    sr, data = wavfile.read(path+files)\n",
    "    data = data.tolist()\n",
    "    if r < 0.3:\n",
    "        background_files = path + '_background_noise_/'\n",
    "        choice = random.choice(background_noises)\n",
    "        sr_1, data_1 = wavfile.read(background_files + choice)\n",
    "        data.extend(data_1)\n",
    "    else:\n",
    "        data_1 = [0]*32000\n",
    "        data.extend(data_1)\n",
    "        data = data[:48000]\n",
    "        if len(data) != 48000:\n",
    "            data.extend([0]*(48000-len(data)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_command(new_split=True, speech_setup=speech_setup):\n",
    "    speech_split = {'train.txt':[], 'test.txt':[], 'valid.txt':[]}\n",
    "    for file in speech_split.keys():\n",
    "        if os.path.exists(speech_setup+file):\n",
    "#             os.remove(speech_setup+file)\n",
    "#             split_speech_cmd = True\n",
    "            new_split = False\n",
    "            with open(file) as f:\n",
    "                for line in f:\n",
    "                    speech_split[key].append(line.strip())\n",
    "    speech_cmd = {'train':[[],[]], 'valid':[[],[]], 'test':[[],[]]}\n",
    "    to_test = True\n",
    "    for folder in speech_files:\n",
    "        files = os.listdir(speech_path+folder)\n",
    "        for file in tqdm(files):\n",
    "            file_path = folder+'/'+file\n",
    "            if new_split == False and os.path.isdir(speech_setup_data):\n",
    "                sr, data = wavfile.read(speech_path, file_path)\n",
    "#             if save_processed_speech_cmd_files and new_split:\n",
    "#                 wavfile.write(speech_setup_data+file_path, 16000, np.array(data, dtype=np.int16))\n",
    "            if new_split:\n",
    "                r = random.random()\n",
    "                if r <= 0.7:\n",
    "                    if TRAIN == 'speech_cmd':\n",
    "                        data = make_3_second(speech_path, file_path)\n",
    "                        speech_split['train.txt'].append(file_path)\n",
    "                        speech_cmd['train'][0].append(data[:48000])\n",
    "                        speech_cmd['train'][1].append(folder)\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    data = make_3_second(speech_path, file_path)\n",
    "                    if to_test:\n",
    "                        to_test = False\n",
    "                        speech_split['test.txt'].append(file_path)\n",
    "                        speech_cmd['test'][0].append(data[:48000])\n",
    "                        speech_cmd['test'][1].append(folder)\n",
    "                    else:\n",
    "                        to_test = True\n",
    "                        speech_split['valid.txt'].append(file_path)\n",
    "                        speech_cmd['valid'][0].append(data[:48000])\n",
    "                        speech_cmd['valid'][1].append(folder)\n",
    "            else:\n",
    "                if (file_path in speech_split['train.txt']):\n",
    "                    if TRAIN == 'speech_cmd':\n",
    "                        speech_cmd['train'][0].append(data[:48000])\n",
    "                        speech_cmd['train'][1].append(folder)\n",
    "                elif file_path in speech_split['valid.txt']:\n",
    "                    speech_cmd['valid'][0].append(data)\n",
    "                    speech_cmd['valid'][1].append(folder)\n",
    "                else:\n",
    "                    speech_cmd['test'][0].append(data)\n",
    "                    speech_cmd['test'][1].append(folder)\n",
    "    return speech_split, speech_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2372/2372 [00:16<00:00, 143.45it/s]\n",
      "100%|██████████| 2380/2380 [00:14<00:00, 160.83it/s]\n",
      "100%|██████████| 2377/2377 [00:17<00:00, 137.52it/s]\n",
      "100%|██████████| 2375/2375 [00:18<00:00, 131.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_split, speech_cmd = speech_command()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in speech_cmd.keys():\n",
    "    speech_cmd[key][0] = np.array(speech_cmd[key][0])\n",
    "    speech_cmd[key][1] = np.array(speech_cmd[key][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : (0,)\n",
      "valid : (1445, 48000)\n",
      "test : (1446, 48000)\n"
     ]
    }
   ],
   "source": [
    "for key in speech_cmd.keys():\n",
    "    print(key, \":\", speech_cmd[key][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for file in speech_split.keys():\n",
    "    with open('speech_'+file, 'w') as f:\n",
    "        for item in speech_split[file]:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rakshak_dataset(new_split=True, rakshak_path=rakshak_path, datatype='keyword'):\n",
    "    rakshak_split = {'rakshak_train.txt':[], 'rakshak_test.txt':[], 'rakshak_valid.txt':[]}\n",
    "    for file in rakshak_split.keys():\n",
    "        if os.path.exists(rakshak_path+file):\n",
    "#             os.remove(datapath+file)\n",
    "#             new_split = True\n",
    "            new_split = False\n",
    "            with open(file) as f:\n",
    "                for line in f:\n",
    "                    rakshak_split[key].append(line.strip())\n",
    "    rakshak = {'train':[[],[]], 'valid':[[],[]], 'test':[[],[]]}\n",
    "    to_test = True\n",
    "    idx = {'keyword':0, 'background':1, 'stress':2}\n",
    "    datapath = rakshak_path + 'paperdata/'\n",
    "    for folder in os.listdir(datapath):\n",
    "        files = os.listdir(datapath+folder)\n",
    "        for file in tqdm(files):\n",
    "            file_path = folder+'/'+file\n",
    "            sr, data = wavfile.read(datapath+file_path)\n",
    "            if data.shape[0] < 48000:\n",
    "                data = np.append(data, [0]*(48000-data.shape[0]))\n",
    "            if new_split:\n",
    "                r = random.random()\n",
    "                if file.split('-')[idx[datatype]] in speech_files:\n",
    "                    if r <= 0.7:\n",
    "                        if TRAIN != 'rakshak':\n",
    "                            continue\n",
    "                        rakshak_split['rakshak_train.txt'].append(file_path)\n",
    "                        rakshak['train'][0].append(data[:48000])\n",
    "                        rakshak['train'][1].append(file.split('-')[idx[datatype]])\n",
    "                        if use_augmentation:\n",
    "                            rakshak['train'][0].append(roll(data)[:48000])\n",
    "                            rakshak['train'][1].append(file.split('-')[idx[datatype]])\n",
    "                            rakshak['train'][0].append(stretch(data)[:48000])\n",
    "                            rakshak['train'][1].append(file.split('-')[idx[datatype]])\n",
    "                            rakshak['train'][0].append(wnoise(data)[:48000])\n",
    "                            rakshak['train'][1].append(file.split('-')[idx[datatype]])\n",
    "                    else:\n",
    "                        if to_test:\n",
    "                            to_test = False\n",
    "                            rakshak_split['rakshak_test.txt'].append(file_path)\n",
    "                            rakshak['test'][0].append(data[:48000])\n",
    "                            rakshak['test'][1].append(file.split('-')[idx[datatype]])\n",
    "                        else:\n",
    "                            to_test = True\n",
    "                            rakshak_split['rakshak_valid.txt'].append(file_path)\n",
    "                            rakshak['valid'][0].append(data[:48000])\n",
    "                            rakshak['valid'][1].append(file.split('-')[idx[datatype]])\n",
    "            else:\n",
    "                if file_path in rakshak_split['rakshak_train.txt']:\n",
    "                    if TRAIN != 'rakshak':\n",
    "                        rakshak['train'][0].append(data[:48000])\n",
    "                        rakshak['train'][1].append(file.split('-')[idx[datatype]])\n",
    "                        if use_augmentation:\n",
    "                            rakshak['train'][0].append(roll(data)[:48000])\n",
    "                            rakshak['train'][1].append(file.split('-')[idx[datatype]])\n",
    "                            rakshak['train'][0].append(stretch(data)[:48000])\n",
    "                            rakshak['train'][1].append(file.split('-')[idx[datatype]])\n",
    "                            rakshak['train'][0].append(wnoise(data)[:48000])\n",
    "                            rakshak['train'][1].append(file.split('-')[idx[datatype]])\n",
    "                elif file_path in rakshak_split['rakshak_valid.txt']:\n",
    "                    rakshak['valid'][0].append(data[:48000])\n",
    "                    rakshak['valid'][1].append(file.split('-')[idx[datatype]])\n",
    "                elif file_path in rakshak_split['rakshak_test.txt']:\n",
    "                    rakshak['test'][0].append(data[:48000])\n",
    "                    rakshak['test'][1].append(file.split('-')[idx[datatype]])\n",
    "    return rakshak_split, rakshak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:00<00:00, 11096.04it/s]\n",
      "100%|██████████| 707/707 [00:09<00:00, 76.26it/s]\n",
      "100%|██████████| 29/29 [00:00<00:00, 10159.94it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 9242.63it/s]\n",
      "100%|██████████| 145/145 [00:01<00:00, 99.34it/s] \n",
      "100%|██████████| 79/79 [00:00<00:00, 13027.84it/s]\n",
      "100%|██████████| 39/39 [00:00<00:00, 96.80it/s]\n",
      "100%|██████████| 385/385 [00:03<00:00, 128.22it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 1538.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21411"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rakshak_split, rakshak = rakshak_dataset()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in rakshak.keys():\n",
    "    rakshak[key][0] = np.array(rakshak[key][0])\n",
    "    rakshak[key][1] = np.array(rakshak[key][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : (2440, 48000)\n",
      "valid : (121, 48000)\n",
      "test : (122, 48000)\n"
     ]
    }
   ],
   "source": [
    "for key in rakshak.keys():\n",
    "    print(key, \":\", rakshak[key][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (2562, 48000)\n",
      "labels shape:  (2562,)\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_RAKSHAK_ON_ALL_DATA:\n",
    "    all_rakshak_data = np.vstack((rakshak['train'][0], rakshak['test'][0]))\n",
    "    all_rakshak_labels = []\n",
    "    for key in rakshak.keys():\n",
    "        if key == 'valid':\n",
    "            continue\n",
    "        all_rakshak_labels.extend(rakshak[key][1])\n",
    "    all_rakshak_labels = np.array(all_rakshak_labels)\n",
    "    print(\"data shape: \", all_rakshak_data.shape)\n",
    "    print(\"labels shape: \", all_rakshak_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for file in rakshak_split.keys():\n",
    "    with open(file, 'w') as f:\n",
    "        for item in rakshak_split[file]:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'go': 0, 'no': 1, 'stop': 2, 'yes': 3}\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "if TRAIN == 'speech_cmd':\n",
    "    speech_cmd['train'][0] = speech_cmd['train'][0].reshape(speech_cmd['train'][0].shape[0], 1, 16000 * 3)\n",
    "    speech_cmd['test'][0] = speech_cmd['test'][0].reshape(speech_cmd['test'][0].shape[0], 1, 16000 * 3)\n",
    "    speech_cmd['valid'][0] = speech_cmd['valid'][0].reshape(speech_cmd['valid'][0].shape[0], 1, 16000 * 3)\n",
    "    rakshak['test'][0] = rakshak['test'][0].reshape(rakshak['test'][0].shape[0], 1, 16000 * 3)\n",
    "    rakshak['valid'][0] = rakshak['valid'][0].reshape(rakshak['valid'][0].shape[0], 1, 16000 * 3)\n",
    "    speech_cmd['train'][1] = labelencoder.fit_transform(speech_cmd['train'][1])\n",
    "    speech_cmd['test'][1] = labelencoder.transform(speech_cmd['test'][1])\n",
    "    speech_cmd['valid'][1] = labelencoder.transform(speech_cmd['valid'][1])\n",
    "    rakshak['test'][1] = labelencoder.transform(rakshak['test'][1])\n",
    "    rakshak['valid'][1] = labelencoder.transform(rakshak['valid'][1])\n",
    "    mapping = dict(zip(labelencoder.classes_, range(len(labelencoder.classes_))))\n",
    "    speech_cmd['train'][1] = to_categorical(speech_cmd['train'][1])\n",
    "    speech_cmd['test'][1] = to_categorical(speech_cmd['test'][1])\n",
    "    speech_cmd['valid'][1] = to_categorical(speech_cmd['valid'][1])\n",
    "    rakshak['test'][1] = to_categorical(rakshak['test'][1])\n",
    "    rakshak['valid'][1] = to_categorical(rakshak['valid'][1])\n",
    "else:\n",
    "    rakshak['train'][0] = rakshak['train'][0].reshape(rakshak['train'][0].shape[0], 1, 16000 * 3)\n",
    "    rakshak['test'][0] = rakshak['test'][0].reshape(rakshak['test'][0].shape[0], 1, 16000 * 3)\n",
    "    rakshak['valid'][0] = rakshak['valid'][0].reshape(rakshak['valid'][0].shape[0], 1, 16000 * 3)\n",
    "    speech_cmd['test'][0] = speech_cmd['test'][0].reshape(speech_cmd['test'][0].shape[0], 1, 16000 * 3)\n",
    "    speech_cmd['valid'][0] = speech_cmd['valid'][0].reshape(speech_cmd['valid'][0].shape[0], 1, 16000 * 3)\n",
    "    rakshak['train'][1] = labelencoder.fit_transform(rakshak['train'][1])\n",
    "    rakshak['test'][1] = labelencoder.transform(rakshak['test'][1])\n",
    "    rakshak['valid'][1] = labelencoder.transform(rakshak['valid'][1])\n",
    "    speech_cmd['test'][1] = labelencoder.transform(speech_cmd['test'][1])\n",
    "    speech_cmd['valid'][1] = labelencoder.transform(speech_cmd['valid'][1])\n",
    "    mapping = dict(zip(labelencoder.classes_, range(len(labelencoder.classes_))))\n",
    "    rakshak['train'][1] = to_categorical(rakshak['train'][1])\n",
    "    rakshak['test'][1] = to_categorical(rakshak['test'][1])\n",
    "    rakshak['valid'][1] = to_categorical(rakshak['valid'][1])\n",
    "    speech_cmd['test'][1] = to_categorical(speech_cmd['test'][1])\n",
    "    speech_cmd['valid'][1] = to_categorical(speech_cmd['valid'][1])\n",
    "    if TRAIN_RAKSHAK_ON_ALL_DATA:\n",
    "        all_rakshak_data = all_rakshak_data.reshape(all_rakshak_data.shape[0], 1, 16000*3)\n",
    "        all_rakshak_labels = labelencoder.transform(all_rakshak_labels)\n",
    "        all_rakshak_labels = to_categorical(all_rakshak_labels)\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'go', 1: 'no', 2: 'stop', 3: 'yes'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_mapping = dict(map(reversed, mapping.items()))\n",
    "inverted_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import kapre\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,AveragePooling2D\n",
    "from kapre.time_frequency import Melspectrogram\n",
    "from kapre.utils import Normalization2D\n",
    "from kapre.augmentation import AdditiveNoise\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Dense, Dropout, Flatten, Conv2D, MaxPooling2D,SeparableConv2D,BatchNormalization,LSTM,Reshape,TimeDistributed\n",
    "\n",
    "\n",
    "# 6 channels (!), maybe 1-sec audio signal, for an example.\n",
    "\n",
    "sr = 16000\n",
    "input_shape = (1,sr*3)\n",
    "\n",
    "\n",
    "def edgespeechneta():\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\"logs/esna\")\n",
    "    model = Sequential()\n",
    "    # A mel-spectrogram layer\n",
    "    model.add(Melspectrogram(n_dft=512, n_hop=512, input_shape=input_shape,\n",
    "                             padding='same', sr=sr, n_mels=128,\n",
    "                             fmin=0.0, fmax=sr/2, power_melgram=1.0,\n",
    "                             return_decibel_melgram=True,trainable_fb=False,\n",
    "                             trainable_kernel=False,\n",
    "                             name='trainable_stft'))\n",
    "    # Maybe some additive white noise.\n",
    "    model.add(AdditiveNoise(power=0.1))\n",
    "    # If you wanna normalise it per-frequency\n",
    "    model.add(Normalization2D(str_axis='freq')) # or 'channel', 'time', 'batch', 'data_sample'\n",
    "    # After this, it's just a usual keras workflow. For example..\n",
    "    # Add some layers, e.g., model.add(some convolution layers..)\n",
    "   \n",
    "    # Compile the model\n",
    "    model.add(Conv2D(39, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(Conv2D(20, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(Conv2D(39, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(15, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(Conv2D(39, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(25, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(Conv2D(39, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(22, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(Conv2D(39, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(22, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(Conv2D(39, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(25, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(Conv2D(39, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(Conv2D(45, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def edgespeechnetb():\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\"logs/edgespeechnet-b-100e\")\n",
    "    model = Sequential()\n",
    "    # A mel-spectrogram layer\n",
    "    model.add(Melspectrogram(n_dft=512, n_hop=512, input_shape=input_shape,\n",
    "                             padding='same', sr=sr, n_mels=128,\n",
    "                             fmin=0.0, fmax=sr/2, power_melgram=1.0,\n",
    "                             return_decibel_melgram=True,trainable_fb=False,\n",
    "                             trainable_kernel=False,\n",
    "                             name='trainable_stft'))\n",
    "    # Maybe some additive white noise.\n",
    "    model.add(AdditiveNoise(power=0.1))\n",
    "    # If you wanna normalise it per-frequency\n",
    "    model.add(Normalization2D(str_axis='freq')) # or 'channel', 'time', 'batch', 'data_sample'\n",
    "    # After this, it's just a usual keras workflow. For example..\n",
    "    # Add some layers, e.g., model.add(some convolution layers..)\n",
    "   \n",
    "    # Compile the model\n",
    "    model.add(Conv2D(45, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(30, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(Conv2D(45, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(Conv2D(33, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(Conv2D(45, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(Conv2D(35, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(Conv2D(45, kernel_size=(3, 3), activation='relu',dim_ordering=\"th\"))\n",
    "\n",
    "    model.add(AveragePooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def cnn():\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\"logs/cnn-100e\")\n",
    "    model = Sequential()\n",
    "    # A mel-spectrogram layer\n",
    "    model.add(Melspectrogram(n_dft=512, n_hop=512, input_shape=input_shape,\n",
    "                             padding='same', sr=sr, n_mels=128,\n",
    "                             fmin=0.0, fmax=sr/2, power_melgram=1.0,\n",
    "                             return_decibel_melgram=True,trainable_fb=False,\n",
    "                             trainable_kernel=False,\n",
    "                             name='trainable_stft'))\n",
    "    # Maybe some additive white noise.\n",
    "    model.add(AdditiveNoise(power=0.1))\n",
    "    # If you wanna normalise it per-frequency\n",
    "    model.add(Normalization2D(str_axis='freq')) # or 'channel', 'time', 'batch', 'data_sample'\n",
    "    # After this, it's just a usual keras workflow. For example..\n",
    "    # Add some layers, e.g., model.add(some convolution layers..)\n",
    "    # Compile the model\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(20, 8), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),dim_ordering=\"th\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=(10, 4), activation='relu',dim_ordering=\"th\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),dim_ordering=\"th\"))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "model = edgespeechnetb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "trainable_stft (Melspectrogr (None, 128, 94, 1)        296064    \n",
      "_________________________________________________________________\n",
      "additive_noise_1 (AdditiveNo (None, 128, 94, 1)        0         \n",
      "_________________________________________________________________\n",
      "normalization2d_1 (Normaliza (None, 128, 94, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 126, 92, 45)       450       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 63, 46, 45)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 44, 43)        17040     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 45, 42, 41)        12195     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 33, 40, 39)        13398     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 45, 38, 37)        13410     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 35, 36, 35)        14210     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 45, 34, 33)        14220     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 22, 17, 33)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12342)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 49372     \n",
      "=================================================================\n",
      "Total params: 430,359\n",
      "Trainable params: 134,295\n",
      "Non-trainable params: 296,064\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2562 samples, validate on 121 samples\n",
      "Epoch 1/50\n",
      "2562/2562 [==============================] - 5s 2ms/step - loss: 1.3306 - acc: 0.3458 - val_loss: 1.1769 - val_acc: 0.3719\n",
      "Epoch 2/50\n",
      "2562/2562 [==============================] - 2s 636us/step - loss: 1.1776 - acc: 0.4684 - val_loss: 1.0520 - val_acc: 0.5620\n",
      "Epoch 3/50\n",
      "2562/2562 [==============================] - 2s 634us/step - loss: 0.9718 - acc: 0.5648 - val_loss: 0.7803 - val_acc: 0.5950\n",
      "Epoch 4/50\n",
      "2562/2562 [==============================] - 2s 624us/step - loss: 0.7085 - acc: 0.6690 - val_loss: 0.7240 - val_acc: 0.7025\n",
      "Epoch 5/50\n",
      "2562/2562 [==============================] - 2s 616us/step - loss: 0.5461 - acc: 0.7521 - val_loss: 0.7889 - val_acc: 0.7107\n",
      "Epoch 6/50\n",
      "2562/2562 [==============================] - 2s 617us/step - loss: 0.4918 - acc: 0.7834 - val_loss: 0.6802 - val_acc: 0.7521\n",
      "Epoch 7/50\n",
      "2562/2562 [==============================] - 2s 620us/step - loss: 0.4021 - acc: 0.8372 - val_loss: 0.6726 - val_acc: 0.7934\n",
      "Epoch 8/50\n",
      "2562/2562 [==============================] - 2s 624us/step - loss: 0.2861 - acc: 0.8891 - val_loss: 0.8307 - val_acc: 0.7934\n",
      "Epoch 9/50\n",
      "2562/2562 [==============================] - 2s 625us/step - loss: 0.2712 - acc: 0.8973 - val_loss: 0.6097 - val_acc: 0.8182\n",
      "Epoch 10/50\n",
      "2562/2562 [==============================] - 2s 625us/step - loss: 0.1917 - acc: 0.9231 - val_loss: 0.6395 - val_acc: 0.7934\n",
      "Epoch 11/50\n",
      "2562/2562 [==============================] - 2s 614us/step - loss: 0.1388 - acc: 0.9481 - val_loss: 1.1413 - val_acc: 0.7851\n",
      "Epoch 12/50\n",
      "2562/2562 [==============================] - 2s 620us/step - loss: 0.1410 - acc: 0.9477 - val_loss: 0.9038 - val_acc: 0.8182\n",
      "Epoch 13/50\n",
      "2562/2562 [==============================] - 2s 626us/step - loss: 0.1107 - acc: 0.9598 - val_loss: 1.0393 - val_acc: 0.8264\n",
      "Epoch 14/50\n",
      "2562/2562 [==============================] - 2s 614us/step - loss: 0.1094 - acc: 0.9625 - val_loss: 1.2737 - val_acc: 0.8099\n",
      "Epoch 15/50\n",
      "2562/2562 [==============================] - 2s 622us/step - loss: 0.0806 - acc: 0.9688 - val_loss: 1.0390 - val_acc: 0.8182\n",
      "Epoch 16/50\n",
      "2562/2562 [==============================] - 2s 619us/step - loss: 0.0942 - acc: 0.9664 - val_loss: 0.5886 - val_acc: 0.8347\n",
      "Epoch 17/50\n",
      "2562/2562 [==============================] - 2s 619us/step - loss: 0.0558 - acc: 0.9793 - val_loss: 1.1343 - val_acc: 0.8347\n",
      "Epoch 18/50\n",
      "2562/2562 [==============================] - 2s 615us/step - loss: 0.0876 - acc: 0.9692 - val_loss: 1.2362 - val_acc: 0.8182\n",
      "Epoch 19/50\n",
      "2562/2562 [==============================] - 2s 622us/step - loss: 0.0752 - acc: 0.9719 - val_loss: 0.8418 - val_acc: 0.8347\n",
      "Epoch 20/50\n",
      "2562/2562 [==============================] - 2s 622us/step - loss: 0.0498 - acc: 0.9856 - val_loss: 0.9489 - val_acc: 0.8430\n",
      "Epoch 21/50\n",
      "2562/2562 [==============================] - 2s 627us/step - loss: 0.0360 - acc: 0.9863 - val_loss: 1.2125 - val_acc: 0.8430\n",
      "Epoch 22/50\n",
      "2562/2562 [==============================] - 2s 623us/step - loss: 0.0444 - acc: 0.9879 - val_loss: 0.9069 - val_acc: 0.8512\n",
      "Epoch 23/50\n",
      "2562/2562 [==============================] - 2s 623us/step - loss: 0.0712 - acc: 0.9789 - val_loss: 1.2631 - val_acc: 0.8099\n",
      "Epoch 24/50\n",
      "2562/2562 [==============================] - 2s 618us/step - loss: 0.0658 - acc: 0.9770 - val_loss: 0.9652 - val_acc: 0.8512\n",
      "Epoch 25/50\n",
      "2562/2562 [==============================] - 2s 625us/step - loss: 0.0269 - acc: 0.9930 - val_loss: 1.0120 - val_acc: 0.8678\n",
      "Epoch 26/50\n",
      "2562/2562 [==============================] - 2s 622us/step - loss: 0.0286 - acc: 0.9910 - val_loss: 0.9688 - val_acc: 0.8347\n",
      "Epoch 27/50\n",
      "2562/2562 [==============================] - 2s 622us/step - loss: 0.0300 - acc: 0.9887 - val_loss: 1.3789 - val_acc: 0.8264\n",
      "Epoch 28/50\n",
      "2562/2562 [==============================] - 2s 624us/step - loss: 0.0361 - acc: 0.9887 - val_loss: 0.9368 - val_acc: 0.8512\n",
      "Epoch 29/50\n",
      "2562/2562 [==============================] - 2s 628us/step - loss: 0.0212 - acc: 0.9930 - val_loss: 1.4973 - val_acc: 0.8264\n",
      "Epoch 30/50\n",
      "2562/2562 [==============================] - 2s 621us/step - loss: 0.0152 - acc: 0.9953 - val_loss: 1.1852 - val_acc: 0.8430\n",
      "Epoch 31/50\n",
      "2562/2562 [==============================] - 2s 620us/step - loss: 0.0128 - acc: 0.9953 - val_loss: 1.6485 - val_acc: 0.8430\n",
      "Epoch 32/50\n",
      "2562/2562 [==============================] - 2s 618us/step - loss: 0.0821 - acc: 0.9727 - val_loss: 0.9195 - val_acc: 0.8512\n",
      "Epoch 33/50\n",
      "2562/2562 [==============================] - 2s 619us/step - loss: 0.0310 - acc: 0.9887 - val_loss: 0.8612 - val_acc: 0.8678\n",
      "Epoch 34/50\n",
      "2562/2562 [==============================] - 2s 636us/step - loss: 0.0075 - acc: 0.9980 - val_loss: 1.3512 - val_acc: 0.8264\n",
      "Epoch 35/50\n",
      "2562/2562 [==============================] - 2s 627us/step - loss: 0.0358 - acc: 0.9863 - val_loss: 1.2497 - val_acc: 0.8182\n",
      "Epoch 36/50\n",
      "2562/2562 [==============================] - 2s 620us/step - loss: 0.0068 - acc: 0.9980 - val_loss: 1.2401 - val_acc: 0.8430\n",
      "Epoch 37/50\n",
      "2562/2562 [==============================] - 2s 622us/step - loss: 0.0169 - acc: 0.9934 - val_loss: 1.0139 - val_acc: 0.8512\n",
      "Epoch 38/50\n",
      "2562/2562 [==============================] - 2s 619us/step - loss: 0.0304 - acc: 0.9887 - val_loss: 0.6723 - val_acc: 0.8678\n",
      "Epoch 39/50\n",
      "2562/2562 [==============================] - 2s 619us/step - loss: 0.0083 - acc: 0.9988 - val_loss: 1.0116 - val_acc: 0.8678\n",
      "Epoch 40/50\n",
      "2562/2562 [==============================] - 2s 623us/step - loss: 0.0479 - acc: 0.9836 - val_loss: 1.3438 - val_acc: 0.8264\n",
      "Epoch 41/50\n",
      "2562/2562 [==============================] - 2s 618us/step - loss: 0.0445 - acc: 0.9852 - val_loss: 0.4959 - val_acc: 0.8926\n",
      "Epoch 42/50\n",
      "2562/2562 [==============================] - 2s 621us/step - loss: 0.0097 - acc: 0.9965 - val_loss: 1.0009 - val_acc: 0.8843\n",
      "Epoch 43/50\n",
      "2562/2562 [==============================] - 2s 624us/step - loss: 0.0268 - acc: 0.9918 - val_loss: 1.0652 - val_acc: 0.8430\n",
      "Epoch 44/50\n",
      "2562/2562 [==============================] - 2s 618us/step - loss: 0.0132 - acc: 0.9973 - val_loss: 1.0194 - val_acc: 0.8678\n",
      "Epoch 45/50\n",
      "2562/2562 [==============================] - 2s 620us/step - loss: 0.0020 - acc: 0.9992 - val_loss: 1.0383 - val_acc: 0.8760\n",
      "Epoch 46/50\n",
      "2562/2562 [==============================] - 2s 622us/step - loss: 0.0204 - acc: 0.9949 - val_loss: 0.8202 - val_acc: 0.8678\n",
      "Epoch 47/50\n",
      "2562/2562 [==============================] - 2s 723us/step - loss: 0.0157 - acc: 0.9949 - val_loss: 0.5501 - val_acc: 0.8843\n",
      "Epoch 48/50\n",
      "2562/2562 [==============================] - 2s 647us/step - loss: 0.0092 - acc: 0.9965 - val_loss: 0.9067 - val_acc: 0.8512\n",
      "Epoch 49/50\n",
      "2562/2562 [==============================] - 2s 633us/step - loss: 0.0098 - acc: 0.9965 - val_loss: 1.1694 - val_acc: 0.8430\n",
      "Epoch 50/50\n",
      "2562/2562 [==============================] - 2s 621us/step - loss: 0.0117 - acc: 0.9961 - val_loss: 0.8763 - val_acc: 0.8595\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.adam(),metrics=['accuracy'],)\n",
    "if TRAIN == 'speech_cmd':\n",
    "    model.fit(speech_cmd['train'][0], speech_cmd['train'][1], batch_size=100, epochs=50, verbose=1, validation_data=(speech_cmd['valid'][0], speech_cmd['valid'][1]))\n",
    "else:\n",
    "    model.fit(all_rakshak_data, all_rakshak_labels, batch_size=50, epochs=50, verbose=1, validation_data=(rakshak['valid'][0], rakshak['valid'][1]))\n",
    "model.save('rakshak/esnb_{}.h5'.format(TRAIN))\n",
    "# model.fit(all_rakshak_data, all_rakshak_labels, batch_size=50, epochs=50, verbose=1, validation_data=(speech_cmd['valid'][0], speech_cmd['valid'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speech_cmd_test(which):\n",
    "    colors = {'rakshak':'#ff7979', 'speech_cmd':'#f6e58d'}\n",
    "    if 'speech_cmd' in TEST:\n",
    "        from keras.models import load_model\n",
    "        model1 = load_model('rakshak/esnb_{}.h5'.format(which), custom_objects={'Melspectrogram':kapre.time_frequency.Melspectrogram, 'AdditiveNoise':kapre.augmentation.AdditiveNoise,\\\n",
    "                                                 'Normalization2D':kapre.utils.Normalization2D})\n",
    "        accuracy ={'valid':{}, 'test':{}}\n",
    "        count = 0\n",
    "        preds_valid = model1.predict(speech_cmd['valid'][0])\n",
    "        preds_test = model1.predict(speech_cmd['test'][0])\n",
    "        for i in range(preds_valid.shape[0]):\n",
    "            if np.argmax(preds_valid[i]) == np.argmax(speech_cmd['valid'][1][i]):\n",
    "                count += 1\n",
    "                if inverted_mapping[np.argmax(preds_valid[i])] not in accuracy['valid'].keys():\n",
    "    #                 print(inverted_mapping[np.argmax(preds_valid[i])])\n",
    "                    accuracy['valid'][inverted_mapping[np.argmax(preds_valid[i])]] = 1\n",
    "                else:\n",
    "                    accuracy['valid'][inverted_mapping[np.argmax(preds_valid[i])]] += 1\n",
    "        acc_valid = count/preds_valid.shape[0]\n",
    "        print(\"accuracy on speech_cmd validation set: \", acc_valid)\n",
    "        count = 0\n",
    "        for i in range(preds_test.shape[0]):\n",
    "            if np.argmax(preds_test[i]) == np.argmax(speech_cmd['test'][1][i]):\n",
    "                count += 1\n",
    "                if inverted_mapping[np.argmax(preds_test[i])] not in accuracy['test'].keys():\n",
    "                    accuracy['test'][inverted_mapping[np.argmax(preds_test[i])]] = 1\n",
    "                else:\n",
    "                    accuracy['test'][inverted_mapping[np.argmax(preds_test[i])]] += 1\n",
    "        acc_test = count/preds_test.shape[0]\n",
    "        print(\"accuracy on speech_cmd test set: \", acc_test)\n",
    "        idx = {'test':{}, 'valid':{}}\n",
    "        for type_ in idx.keys():\n",
    "            for file in speech_split[type_+'.txt']:\n",
    "                key = file.split('/')[0]\n",
    "                if key not in idx[type_].keys():\n",
    "                    idx[type_][key] = 1\n",
    "                else:\n",
    "                    idx[type_][key] += 1\n",
    "        total = {}\n",
    "        for type_ in accuracy.keys():\n",
    "            for key in accuracy[type_].keys():\n",
    "                if key not in total.keys():\n",
    "                    total[key] = accuracy[type_][key]\n",
    "                else:\n",
    "                    total[key] += accuracy[type_][key]\n",
    "                accuracy[type_][key] = accuracy[type_][key]/idx[type_][key]\n",
    "        for key in total.keys():\n",
    "            total[key] = total[key]/(idx['test'][key]+idx['valid'][key])\n",
    "        gc.collect()\n",
    "        trace = go.Bar(\n",
    "            x = [*total.keys()],\n",
    "            y = [*total.values()],\n",
    "            marker=dict(\n",
    "                color=colors[which]\n",
    "            ),\n",
    "            name = which\n",
    "        )\n",
    "        return trace\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_plot(trace0, trace1):\n",
    "    if (trace0 is None) or (trace1 is None):\n",
    "        print(\"None data provided\")\n",
    "        return None\n",
    "    else:\n",
    "        data = [trace0, trace1]\n",
    "        layout = go.Layout(\n",
    "            title='accuracy on speech command',\n",
    "        )\n",
    "\n",
    "        fig = go.Figure(data=data, layout=layout)\n",
    "        return py.iplot(fig, filename='accuracy_keywords_on_speech_cmd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int_axis=1 passed but is ignored, str_axis is used instead.\n",
      "accuracy on speech_cmd validation set:  0.9737024221453288\n",
      "accuracy on speech_cmd test set:  0.9785615491009682\n",
      "int_axis=1 passed but is ignored, str_axis is used instead.\n",
      "accuracy on speech_cmd validation set:  0.6512110726643598\n",
      "accuracy on speech_cmd test set:  0.656984785615491\n"
     ]
    }
   ],
   "source": [
    "trace0 = speech_cmd_test('speech_cmd')\n",
    "trace1 = speech_cmd_test('rakshak')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model comparision on Speech Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ujjwal999/8.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<chart_studio.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_plot(trace0, trace1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rakshak place wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8677685950413223\n",
      "number of samples of different places:\n",
      " {'house': 79, 'india gate lake area ': 12, 'railway station': 4, 'quickspeech-in.appspot.com': 23, 'india gate': 3}\n",
      "accuracy based on locations:\n",
      " {'house': 0.8987341772151899, 'india gate lake area ': 0.8333333333333334, 'railway station': 0.5, 'quickspeech-in.appspot.com': 0.8260869565217391, 'india gate': 1.0}\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "preds = model.predict(rakshak['valid'][0])\n",
    "for i in range(preds.shape[0]):\n",
    "    if np.argmax(preds[i]) == np.argmax(rakshak['valid'][1][i]):\n",
    "        count += 1\n",
    "print(\"accuracy: \", count/preds.shape[0])\n",
    "idx = {}\n",
    "length_dict = {}\n",
    "for i,file in enumerate(rakshak_split['rakshak_valid.txt']):\n",
    "    key = file.split('/')[0].lower()\n",
    "    if key not in idx.keys():\n",
    "        idx[key] = [i]\n",
    "        length_dict[key] = 1\n",
    "    else:\n",
    "        idx[key].append(i)\n",
    "        length_dict[key] += 1\n",
    "print(\"number of samples of different places:\\n\", length_dict)\n",
    "accuracy = {}\n",
    "for key in idx.keys():\n",
    "    count = 0\n",
    "    for ids in idx[key]:\n",
    "        if np.argmax(preds[ids]) == np.argmax(rakshak['valid'][1][ids]):\n",
    "            count += 1\n",
    "    accuracy[key] = count/len(idx[key])\n",
    "print(\"accuracy based on locations:\\n\", accuracy)\n",
    "with open(rakshak_path+TRAIN+'_placewise.txt', 'w') as f:\n",
    "    for key in accuracy.keys():\n",
    "        f.write(\"{}:{}\\n\".format(key, accuracy[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'house': 0.8987341772151899,\n",
       " 'india gate lake area ': 0.8333333333333334,\n",
       " 'railway station': 0.5,\n",
       " 'quickspeech-in.appspot.com': 0.8260869565217391,\n",
       " 'india gate': 1.0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_rakshak = {}\n",
    "content_speech = {}\n",
    "with open(rakshak_path+'rakshak_placewise.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        content_rakshak[line.split(\":\")[0].strip()] = line.split(\":\")[1].strip()\n",
    "with open(rakshak_path+'speech_cmd_placewise.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        content_speech[line.split(\":\")[0].strip()] = line.split(\":\")[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~ujjwal999/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<chart_studio.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace0 = go.Bar(\n",
    "    x = [*content_rakshak.keys()],\n",
    "    y = [*content_rakshak.values()],\n",
    "    marker=dict(\n",
    "        color='#ff7979'\n",
    "    ),\n",
    "    name = 'rakshak'\n",
    ")\n",
    "\n",
    "trace1 = go.Bar(\n",
    "    x = [*content_speech.keys()],\n",
    "    y = [*content_speech.values()],\n",
    "    marker=dict(\n",
    "        color='#f6e58d'\n",
    "    ),\n",
    "    name = 'speech_cmd'\n",
    ")\n",
    "\n",
    "data = [trace0, trace1]\n",
    "layout = go.Layout(\n",
    "    title='accuracy of keywords in different places',\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='accuracy_keywords_rakshak')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
